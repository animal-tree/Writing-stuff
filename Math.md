**Physics:**

$\mathbf{P}((y, h^{(n)}) \sim \mathrm{R}^n(x, h^{(0)}) \vert y)$

where $\mathrm{R}$ is a recursive dynamics model and a random variable, $h$ is the hidden state (the non-observable universe) at any time point up to some conceivable horizon (in order to make $\mathrm{R}$ defined), composed $n$ times. $y$ is the observable universe.

**Causal reasoning:**

$\mathbf{P}((y, h^{(n)}) \sim \mathrm{R}^n(x, h^{(0)}) \vert y, \mathrm{R})$

$x$ is the cause, $y$ is the effect, under some dynamics system $\mathrm{R}$.

**Axiomatic math:**

$\mathbf{P}((y, h^{(n)}) \sim \mathrm{R}^n(x, h^{(0)}) \vert h^{(n)}, n \in \mathcal{N}, \mathrm{R}, x, h^{(0)})$

where $\mathrm{R}$ is the rules of math. $y$ is a theorem or theorem set, $x$ is an axiom or axiom set. $\mathrm{R}$ is Turing-complete due its rewritable memory state $h$. Ordinarily, $\mathrm{R}$'s rules are deterministic, a side effect being that the probability of a theorem $y$ is also deterministic, $1$ or $0$. 

Non-deterministic math, e.g. an "Occam's razor" "theorem", we will refer to as epistemological math, rooted in belief.

**Belief:**

$\mathbf{P}(B \vert E) = \frac{\mathbf{P}(E \vert B)\mathbf{P}(B)}{\mathbf{P}(E)}$

Physics, causality, and math have utility and correlative predictiveness to our memories and senses, but beyond observing what's in front of us, these established beliefs are symbolic references within the brain, and the referents are memories, observations, and beliefs. As René Descartes derived with "*cogito, ergo sum*": "I doubt, therefore I am," we can further deduce inwardly. If I may: 

"I observe, therefore I am observing, therefore I am. 

I observe many referents to my observation, including memories, feelings, and viscera. There's also belief. I believe X, Y, Z. I justify X, Y, Z by reasons, another element of belief, X', Y', Z'."

Each step is deeper into the psyche.

Physics, causality, and math are beliefs. 

Philosophy includes these meta-beliefs as well as others.

Perhaps, philosophically speaking, philosophy is the random variable set encompassing all random variables:

**Philosophy:**

$\langle B \vert (B \in \Omega) \sim \mathbf{P}(B \in \Omega) \rangle$,

though to define it as such would contradict that definition since a random variable set encompassing all random variables would have to be contained within itself, making the definition self-referentially undefined. This would be analogous to defining a word in a dictionary by some other words plus itself. Perhaps contextually useful to make some inferences about its meaning based on the other words but ultimately meaningless since any reference to itself within the definition would point back to a non-converging turtles-all-the-way-down recursion of itself. Furthermore, something about Gödel's incompleteness theorem.

Random variables may be also expressed as random variables relating other random variables, such as $\mathbf{P}(x > y)$.

For the sake of deduction to derive axioms to theorems, they may also define implications:

**Theorem-derivation steps:**

$\mathbf{P}(x > z \vert x > y, y > z) = 1$

Transitivity of the greater-than inequality for example, using conditionals to specify the valid axioms or theorems $(x > y, y > z)$ that would imply the prior $x > z$.

They may even be cross-referential:

$\mathbf{P}(w > z \vert w > x, \mathbf{P}(x > z \vert x > y, y > z) = 1) = 1$,

Treating each other as random variables.

Could such an ontology form the basis of a data structure to represent $R, h^{(0)}$ in axiomatic math? Using deterministic conditionals with probabilities $1$ as proof steps and a defined set of initial conditionals and beliefs (random variables), it seems so.

Furthermore, it can generalize to a new type of math where proof steps can be epistemological, non-deterministic beliefs applied to other beliefs to quantify a looser proof. This is standardly done in physical sciences with the use of p-values, and we draw a formal unambiguous connection between that to AI and reasoning generally, keeping these notations and formalities that tie together the larger sciences.

**MHDPA model:**

We can define this data structure effectively as a transformer, learning a parameterized set of tokens that represent random variables ($h$) and which map to other tokens probabilistically under a shared invariance function ($\mathrm{R}$, represented as MHDPA). That way, a single math can be learned across tokens, with the expressive power of the above formalism verified due to its analogous representation as a transformer.

What we've shown is that a transformer can express the capacity of the above formalism.

This isn't precisely what I'm going for, but it shows some of the connection lines that draw my thinking (learnable random variables, or quantum states, physically interacting with each other).

It can be further generalized to a Vector-Quantizer, long-term memory, and physics model. 

**Larger aim:**

Could these maths be used to deduce theories around epistemological arguments? Such as to say, here is a definiton of Occam's razor, justified epistemlogically by this reasoning principle, and therefore this derivation step? If so, answering some major questions in consciousness may be possible, though the epistemelogical definitions and steps are not shown here. One such question is: "Could consciousness exist in a modern computer system?" With CPUs, GPUs, and TPUs, under some assumptions, such as the infeasibility of a flipbook or holograph to be as conscious as a human given a neuromorphic representation in pen or projection, it's unlikely. My journal is full of fairly-simple math for making this deduction, here presented hand-wavingly, formally in the future.

Theoretical claims about consciousness will require this kind of math.

As well as gnoses about the world.

**For some reason:**

for some reason as I write this, my throat hurts and I am worried I'm not supposed to say it.

There may be an inner wisdom telling me to wait until I've formalized it correctly.

I wish it would do so without stringing my neck and turning the wheel of my chakras far to the right.
