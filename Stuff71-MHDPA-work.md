Note: “a ChatGPT paper” refers to a work I did in 2018, prophetically early for the method, called [“Distributional Relations In Deep Learning.”](https://www.overleaf.com/read/qgmmzgsrctmg#6cd1b9) Back then, the professors at the University of Rochester in majority believed and argued that Deep Learning was a fad. My first advisor, Henry Kautz, was really skeptical of my obsession with this obscure thing called “MHDPA”, the method that later took over Deep Learning and underpins models like ChatGPT. He emailed that he thought I should focus on Neural Architecture Search. I was (somewhat heretically) interested in reinforcement learning, relational reasoning (such as via MHDPA, both for sequences and spatial entities / vision, but note that back then this was an alienating or singling view to take), and long-term lifelong memory (such as the early neural episodic control and memory networks). At the time of that work, I was probably the only one outside of DeepMind who heard of MHDPA and certainly at my university. An author of an MHDPA-based RNN at NeurIPS 2018 was so impressed with me he even said he’d recommend me to his colleagues at DeepMind if I applied, though I of course never did because I was hospitalized and brain damaged around that time. 
 
Back then I had to use Tensorflow 1.0 and there wasn’t an existing infrastructure. I ran the experiments I could in the hospital and got successful results. The reviewers of the paper who rejected it didn’t know what MHDPA was. And they were unsatisfied with the number of experiments, and the jargon about relational reasoning (today, that jargon would not have been controversial). That work and foresight, much like muuuch and maaany of the disciplines and work of my doctoral-program years (and this really can’t be understated), got wasted, often by ill-timed sudden and improbable circumstances, or foolish advisors and environment. 
 
Most recently, my second advisor deciding last-minute out of nowhere in the middle of a paper review (of a paper that had positive reviews and got accepted) that I didn’t have enough papers. And I was terminated before being able to run my robotics / foundation model experiments on the A6000s, literally a week after having finished the core programming for them. [I released that code open source](https://github.com/animal-tree/tributaries/blob/main/Examples/Sweeps/Bittle.py), but have no funding and barely the morale to trust the reader to believe anything I claim

Note: I was also new to Deep Learning when I entered the program. I just learned it faster while balancing the extremely-myopic “breadth” courses I had to take. I graduated my undergrad in May, 2017 and went straight into the PhD program in September with ideas about studying empathy and giving empathy to machines ([it’s not as stupid as it sounds](https://github.com/slerman12/PersonalWebpage/blob/master/IRTG%20Project%20Proposal.pdf)). Then I started Transcendental Meditation and had hypnogogic revelations about ontological entities composing relations querying associative memories, a method of nearest neighbors long-term memory I envisioned in a hypnogogic state that I called “circle waves” that DeepMind turned out to have a similar line on with Neural Episodic Control, but to be clear: [I remained lucid and professional at work](https://github.com/AGI-init/XRDs/tree/main).

I should add: I got an [honorable mention](https://www.research.gov/grfp/AwardeeList.do?method=loadAwardeeList) (type in Samuel Lerman under "Search by Name" and select "All Years") in the NSF GRFP, with all excellent reviews and the only doubt being that (at the time) I didn't have any publications. That is a highly competetive fellowship. Oh, for that very idea that I just qualified with a disclaimer about acting professional.
